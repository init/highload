## Базовый расчёт аппаратных ресурсов
Мы начали расчёт нагрузки с оценки аудиторных показателей будущего проекта и их пересчета в технические метрики приложения в которых измеряется нагрузка такие как RPS, размер данных в хранилищах, потребление пропускной способности сети. После мы разделили проект на доменные зоны в которых выделили сервисы которые будут обслуживать нагрузку.

Теперь наша задача посчитать для каждого сервиса сколько оборудования необходимо чтобы выдерживать планируемую пиковую нагрузку. Для этого нам нужно определить удельное потребление ресурсов сервисом на единицу нагрузки, например CPU на 1 запрос или RAM на 1 экземпляр сервиса и умножить на плановую пиковую нагрузку.

Достовернее всего сопоставить метрики приложения и метрики оборудования на одном экземпляре сервиса под нагрузкой и вывести линейную зависимость между полезной нагрузкой и потреблением ресурсов оборудования. В идеале нужно последовательно увеличивая живую нагрузку весом на балансировщике дойти до предела когда сервер начнет деградировать и зафиксировать максимальную производительность. 

В случае если приложение еще не разработано нужно подобрать подходящий сторонний бенчмарк.

Ниже приведена таблица c грубыми оценками удельного потребления ресурсов на один экземпляр процесса либо на одно ядро:

| Технология| Характер сервиса | RPS | RAM |
| ------ | ----- | ---- |
| Django | тяжелая бизнес-логика | 1 | 1 Гб |
| C++ | тяжелая бизнес-логика | 10 | 100 Mb |
| Django | средняя бизнес-логика | 10 | 100 Mb |
| C++ | средняя бизнес-логика | 100 | 100 Mb |
| Nginx | SSL handshake (CPS) | 500 | 10 Mb |
| Django | легкое JSON API | 500 | 100 Mb |
| С++ | легкое JSON API | 5000 | 10 Mb |

Синтетические бенчмарки различных технологий и фреймворков можно посмотреть по ссылкам:
1. https://www.nginx.com/blog/testing-the-performance-of-nginx-and-nginx-plus-web-servers/
2. https://www.techempower.com/benchmarks/#section=data-r22&test=fortune

Теперь мы можем составить таблицу со списком необходимых ресурсов для всех сервисов проекта, например такую:

| Сервис | Целевая пиковая нагрузка приложения | CPU | RAM | Net |
| ------ | ----------------------------------- | --- | --- | --- |
| newsfeed | 30000 RPS | 3000 | 3 Tb | 10 Gbit/s |
| messanger | 500000 connections | 500 | 5 Тb | 5 Gbit/s |

Как мы видим наши сервисы не умещаются в физические сервера и требуют горизонтального масштабирования.
## Выбор модели развертывания
Первое что нужно нам нужно выбрать это модель развертывания, будем ли мы запускать приложения в оркестрации Kubernetes или напрямую на машинах физических или виртуальных. На текущий момент развертывание stateless приложений в Kubernetes это рекомендованный и общепринятый способ в индустрии.
### Запуск приложений в Kubernetes
Kubernetes это система оркестрации контейнеров которая автоматизирует развертывание облачных приложений упакованных в Docker-контейнеры. Вы указываете количество экземпляров сервиса, требования по распределению экземпляров по датацентрам и система следит за парков ваших сервисов переселяя их с упавших машин на другие. Общение между сервисами происходит при помощи service discovery потому что приложения постоянно переезжают между машинами меняя IP-адреса и порты.

Оркестрация позволяет повысить плотность упаковки сервисов за счет overcommitment и минимизировать количество оборудования необходимое для обеспечения отказоустойчивости. При этом механизм изоляции контейнеров друг от друга предотвращает большинство потенциальных проблем по сравнению с разветыванием неконтейнеризированных приложений.

Для нагруженных сервисов избыточное потребление ресурсов (overhead) связанное с контейнеризацией может быть очень значительным (до 50%). В первую очередь это касается сети так как в базовой конфигурации сетевое взаимодействие контейнеров с внешним миром проходит через дополнительную виртуальную сеть.
#### Распределение ресурсов в Kubernetes
В оркестрации в качестве ресурсов учитываются только количество ядер процессора (может быть дробным) и оперативная память. Сеть приходится контролировать вручную, использование локальных дисков для долговременного хранения не подразумевается потому что контейнеры могут переезжать между серверами. Для каждого ресурса CPU и RAM задается два параметра:
* requests - резервируемое количество ресурса в оркестрации
* limits - максимальное количество ресурса которое разрешено использовать в моменте

В случае если потребление CPU упирается в limits происходит замедление (throttling) квантами по 100мс, что может вызывать существенное повышение времени ответа интерактивных приложений. Поэтому следует не допускать срабатывания throttling для приложений чувствительных к задержке, и обязательно мониторить срабатывание throttling.

В случае если потребление RAM доходит до limits контейнер уничтожается уже известным нам OOM killer. Соответственно этот лимит рекомендуется использовать исключительно как защитный механизм от аномального всплеска использования памяти отдельным экземпляром приложения.
### Запуск приложений без оркестрации
В этом сценарии мы распределяем сервисы по конкретным физическим или виртуальным машинам. Как правило для этого используются системы управления конфигурацией такие как Puppet, Chef, Salt, Ansible. Именно таким способом запускается сам Kubernetes, базы данных, балансировщики нагрузки и системы мониторинга. На случай выхода из строя конкретной машины мы заранее распределяем резервные экземпляры на хосты в другой зоне доступности.
#### Запуск приложений на виртуальных машинах
Рекомендуется для запуска небольших по нагрузке приложений которые не подходят для запуска в оркестрации по следующим возможным причинам:
1. являются stateful, например базами данных
2. являются частью инфраструктуры на которой работает kubernetes и прочие базовые сервисы
3. при запуске в Kubernetes очень большой overhead или плохие квантили время ответа
Уровень изоляции здесь выше чем в оркестрации так как отсутствует CPU throttling и виртуальная сеть. Однако "шумные соседи" на AWS и в этой схеме могут доставлять неудобства.
#### Запуск приложений на физических машинах (bare-metal)
Рекомендуется для запуска высоконагруженных приложений которые страдают от overhead или CPU throttling в контейнеризации и потребляют несколько физических машин целиком. "Шумные соседи" могут влиять только на сеть на уровне сетевого оборудования к которому подключен сервер. Узлы Kubernetes рекомендуется запускать этим способом кроме случаев запуска небольших кластеров размером меньше нескольких физических машин.
### Облачная модель развертывания
Как мы видим из предыдущих пунктов проектам использующим окрестрацию все равно приходится держать часть сервисов на виртуалках при условии самостоятельного разветртывания Kubernetes, баз данных и балансировщиков. Однако при хостинге в облачном провайдере вы можете пользоваться managed услугами и в таком случае достаточно комбинации managed Kubernetes + managed DB + cloud monitoring. 
## Выбор модели хостинга
При выборе хостинга учитывается стоимость, гибкость, надежность и функционал.
### Облачный хостинг
Представители: AWS, Azure, GCP
Достоинства: Большой выбор managed сервисов, сертификации, гибкость
Недостатки: очень дорого, особенно виртуальные машины и сетевой трафик
Не рекомендуется использовать для крупных проектов. Рекомендуется избегать использования пропиетарных облачных managed технологий не имеющих совместииых open source реализаций.
### Дешевая аренда виртуальных машин
Представители: Digital Ocean, Scaleway, VK Cloud, Yandex Cloud
Достоинства: цена, минимально-достаточный набор managed-сервисов, бесплатная сеть
Подходит для большинства небольших и среднего размера проектов.
### Дешевая аренда bare-metal
Представители: Hetzner, Hostkey
Достоинства: дешевые несерверные машины, бесплатная сеть, дешевые GPU
Недостатки: примитивная сеть, долгий provisioning 
Как правило дешевле витруалок за единицу ресурса, самый дешевый хостинг для небольших проектов.
###  Собственное железо
Достоинства: самая низкая удельная стоимость ресурса
Недостатки: добавление ресурсов связано с закупками и занимает месяцы
Рекомендуется для крупных проектов с хорошо прогнозируемым профилем нагрузки.
### Гибрид
Подход рекомендованный в общем случае. Комбинация подходов перечисленных выше. Чтобы использовать сильные и компенсировать слабые стороны. Как правило в облаках берут гибкость, например таких как запуск временных машин для прогона тестов или обслуживания пиковой нагрузки, а от дешевых хостингов берут экономию на хостинге основной нагрузки.

При выборе между виртуальными и физическими серверами следует руководствоваться размером сервиса в полных физических серверах, если получаем 6 и более единиц то лучше остановиться на них. В этом случае мы получим по 2 сервера в 3 ДЦ, падение одного сервера уже не будет отнимать большую долю емкости.
## Конфигурации серверов
В настоящий момент индустрия пришла к массовой закупке серверов одной конфигурации "под Kubernetes", максимум процессорных ядер, много RAM, небольшое количество NVMe дисков. Соотношение ядра CPU/Память для разных типов виртуалок в облаках также отличается незначительно, не более чем на 50%. Учитывая что память довольно дешевая и как правило остается в избытке относительно нагрузки на CPU её мы учитываем в особых случаях большого потребления памяти таких как in-memory базы данных или кэши.

| Сервис | Целевая пиковая нагрузка приложения | CPU | RAM | Net |
| ------ | ----------------------------------- | --- | --- | --- |
| newsfeed | 30000 RPS | 3000 | 3 Tb | 10 Gbit/s |
| messanger | 500000 connections | 500 | 5 Тb | 5 Gbit/s |

Для каждого сервиса подбираем конфигурацию сервера и хостинг. Для покупных серверов считаем амортизацию на 5 лет.

| Название | Хостинг | Конфигурация                       | Cores | Cnt | Покупка | Аренда |
| -------- | ------- | ---------------------------------- | ----- | --- | ------- | ----- |
| kubenode | own     | 2x6338/16x32GB/2xNVMe4T/2x25Gb/s   |    64 |  60 |  $14500 |  $241 | 
| ci       | hetnzer | i5-13500/64GB/2xNVMe512Gb/2x10Gb/s |    14 |   6 |         |   $60 |

Конфигуратор дешевых серверов supermicro:
* https://www.broadberry.eu/rackmount-servers

Для сервисов в оркестрации:

| Сервис    | CPU/r | CPU/l | RAM/r | RAM/l  | Cnt |
| --------- | ----- | ----- | ----- | ------ | --- |
| newsfeed  |     8 |     8 |  8 Gb |  32 Gb | 400 |
| messanger |     8 |    16 | 80 Gb | 120 Gb |  64 |